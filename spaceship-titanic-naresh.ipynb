{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14c93a8",
   "metadata": {
    "papermill": {
     "duration": 0.012933,
     "end_time": "2023-08-06T17:05:38.401704",
     "exception": false,
     "start_time": "2023-08-06T17:05:38.388771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This Notebook is my first ever attempt at a Kaggle competition. It has consistently produced submission scores in the vicinity of 0.8000 and the best rank I have achieved on the leaderboard so far is 379. For other learners like me, I think this notebook may also come in handy as a 'solved example' that covers a host of concepts such as basic data analysis, data cleaning, pre-processing steps including imputation, scaling, encoding, dimensionality reduction, correlation / mutual-info matrix, and multiple classification algorithms including ensemble models, and deep learning / neural networks, and also ways to maximize training data utilization while still minimizing overfitting / generalization error. \n",
    "\n",
    "If you found this interesting or learned something from it, request to please Upvote this Notebook. And of course, I will appreciate any feedback / constructive criticism. Thanks very much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9066deba",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:38.424662Z",
     "iopub.status.busy": "2023-08-06T17:05:38.424257Z",
     "iopub.status.idle": "2023-08-06T17:05:38.439505Z",
     "shell.execute_reply": "2023-08-06T17:05:38.438303Z"
    },
    "papermill": {
     "duration": 0.030209,
     "end_time": "2023-08-06T17:05:38.442269",
     "exception": false,
     "start_time": "2023-08-06T17:05:38.412060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spaceship-titanic/sample_submission.csv\n",
      "/kaggle/input/spaceship-titanic/train.csv\n",
      "/kaggle/input/spaceship-titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe0340b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:38.465291Z",
     "iopub.status.busy": "2023-08-06T17:05:38.464837Z",
     "iopub.status.idle": "2023-08-06T17:05:50.949144Z",
     "shell.execute_reply": "2023-08-06T17:05:50.948057Z"
    },
    "papermill": {
     "duration": 12.499187,
     "end_time": "2023-08-06T17:05:50.951818",
     "exception": false,
     "start_time": "2023-08-06T17:05:38.452631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries, classes, and functions \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from sklearn.decomposition import PCA, TruncatedSVD  \n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold, StratifiedKFold, StratifiedShuffleSplit  \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier  \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score \n",
    "\n",
    "from xgboost import XGBClassifier \n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "# Set random seed \n",
    "SEED = np.random.default_rng().integers(99999)\n",
    "# SEED = 999999 \n",
    "from tensorflow.keras.utils import set_random_seed \n",
    "set_random_seed = SEED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45e942d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:50.975351Z",
     "iopub.status.busy": "2023-08-06T17:05:50.974375Z",
     "iopub.status.idle": "2023-08-06T17:05:51.205083Z",
     "shell.execute_reply": "2023-08-06T17:05:51.203820Z"
    },
    "papermill": {
     "duration": 0.246134,
     "end_time": "2023-08-06T17:05:51.208623",
     "exception": false,
     "start_time": "2023-08-06T17:05:50.962489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n",
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
      "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
      "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
      "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
      "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "\n",
      "   Transported  \n",
      "0        False  \n",
      "1         True  \n",
      "2        False  \n",
      "3        False  \n",
      "4         True   \n",
      " None \n",
      "                count unique             top  freq        mean          std  \\\n",
      "PassengerId     8693   8693         0001_01     1         NaN          NaN   \n",
      "HomePlanet      8492      3           Earth  4602         NaN          NaN   \n",
      "CryoSleep       8476      2           False  5439         NaN          NaN   \n",
      "Cabin           8494   6560         G/734/S     8         NaN          NaN   \n",
      "Destination     8511      3     TRAPPIST-1e  5915         NaN          NaN   \n",
      "Age           8514.0    NaN             NaN   NaN    28.82793    14.489021   \n",
      "VIP             8490      2           False  8291         NaN          NaN   \n",
      "RoomService   8512.0    NaN             NaN   NaN  224.687617   666.717663   \n",
      "FoodCourt     8510.0    NaN             NaN   NaN  458.077203   1611.48924   \n",
      "ShoppingMall  8485.0    NaN             NaN   NaN  173.729169   604.696458   \n",
      "Spa           8510.0    NaN             NaN   NaN  311.138778  1136.705535   \n",
      "VRDeck        8505.0    NaN             NaN   NaN  304.854791  1145.717189   \n",
      "Name            8493   8473  Gollux Reedall     2         NaN          NaN   \n",
      "Transported     8693      2            True  4378         NaN          NaN   \n",
      "\n",
      "              min   25%   50%   75%      max  \n",
      "PassengerId   NaN   NaN   NaN   NaN      NaN  \n",
      "HomePlanet    NaN   NaN   NaN   NaN      NaN  \n",
      "CryoSleep     NaN   NaN   NaN   NaN      NaN  \n",
      "Cabin         NaN   NaN   NaN   NaN      NaN  \n",
      "Destination   NaN   NaN   NaN   NaN      NaN  \n",
      "Age           0.0  19.0  27.0  38.0     79.0  \n",
      "VIP           NaN   NaN   NaN   NaN      NaN  \n",
      "RoomService   0.0   0.0   0.0  47.0  14327.0  \n",
      "FoodCourt     0.0   0.0   0.0  76.0  29813.0  \n",
      "ShoppingMall  0.0   0.0   0.0  27.0  23492.0  \n",
      "Spa           0.0   0.0   0.0  59.0  22408.0  \n",
      "VRDeck        0.0   0.0   0.0  46.0  24133.0  \n",
      "Name          NaN   NaN   NaN   NaN      NaN  \n",
      "Transported   NaN   NaN   NaN   NaN      NaN   \n",
      " PassengerId     8693\n",
      "HomePlanet         3\n",
      "CryoSleep          2\n",
      "Cabin           6560\n",
      "Destination        3\n",
      "Age               80\n",
      "VIP                2\n",
      "RoomService     1273\n",
      "FoodCourt       1507\n",
      "ShoppingMall    1115\n",
      "Spa             1327\n",
      "VRDeck          1306\n",
      "Name            8473\n",
      "Transported        2\n",
      "dtype: int64 \n",
      " PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import training dataset into padas DataFrame & explore \n",
    "train = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv') \n",
    "\n",
    "print(train.head(), '\\n', \n",
    "train.info(), '\\n', \n",
    "train.describe(include='all').T, '\\n', \n",
    "train.nunique() , '\\n', \n",
    "train.isna().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe4e594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:51.232027Z",
     "iopub.status.busy": "2023-08-06T17:05:51.231528Z",
     "iopub.status.idle": "2023-08-06T17:05:51.354722Z",
     "shell.execute_reply": "2023-08-06T17:05:51.353335Z"
    },
    "papermill": {
     "duration": 0.138171,
     "end_time": "2023-08-06T17:05:51.357498",
     "exception": false,
     "start_time": "2023-08-06T17:05:51.219327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4277 entries, 0 to 4276\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   4277 non-null   object \n",
      " 1   HomePlanet    4190 non-null   object \n",
      " 2   CryoSleep     4184 non-null   object \n",
      " 3   Cabin         4177 non-null   object \n",
      " 4   Destination   4185 non-null   object \n",
      " 5   Age           4186 non-null   float64\n",
      " 6   VIP           4184 non-null   object \n",
      " 7   RoomService   4195 non-null   float64\n",
      " 8   FoodCourt     4171 non-null   float64\n",
      " 9   ShoppingMall  4179 non-null   float64\n",
      " 10  Spa           4176 non-null   float64\n",
      " 11  VRDeck        4197 non-null   float64\n",
      " 12  Name          4183 non-null   object \n",
      "dtypes: float64(6), object(7)\n",
      "memory usage: 434.5+ KB\n",
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0013_01      Earth      True  G/3/S  TRAPPIST-1e  27.0  False   \n",
      "1     0018_01      Earth     False  F/4/S  TRAPPIST-1e  19.0  False   \n",
      "2     0019_01     Europa      True  C/0/S  55 Cancri e  31.0  False   \n",
      "3     0021_01     Europa     False  C/1/S  TRAPPIST-1e  38.0  False   \n",
      "4     0023_01      Earth     False  F/5/S  TRAPPIST-1e  20.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \n",
      "0          0.0        0.0           0.0     0.0     0.0   Nelly Carsoning  \n",
      "1          0.0        9.0           0.0  2823.0     0.0    Lerome Peckers  \n",
      "2          0.0        0.0           0.0     0.0     0.0   Sabih Unhearfus  \n",
      "3          0.0     6652.0           0.0   181.0   585.0  Meratz Caltilter  \n",
      "4         10.0        0.0         635.0     0.0     0.0   Brence Harperez   \n",
      " None \n",
      "                count unique          top  freq        mean          std  min  \\\n",
      "PassengerId     4277   4277      0013_01     1         NaN          NaN  NaN   \n",
      "HomePlanet      4190      3        Earth  2263         NaN          NaN  NaN   \n",
      "CryoSleep       4184      2        False  2640         NaN          NaN  NaN   \n",
      "Cabin           4177   3265      G/160/P     8         NaN          NaN  NaN   \n",
      "Destination     4185      3  TRAPPIST-1e  2956         NaN          NaN  NaN   \n",
      "Age           4186.0    NaN          NaN   NaN   28.658146    14.179072  0.0   \n",
      "VIP             4184      2        False  4110         NaN          NaN  NaN   \n",
      "RoomService   4195.0    NaN          NaN   NaN  219.266269   607.011289  0.0   \n",
      "FoodCourt     4171.0    NaN          NaN   NaN  439.484296  1527.663045  0.0   \n",
      "ShoppingMall  4179.0    NaN          NaN   NaN  177.295525   560.821123  0.0   \n",
      "Spa           4176.0    NaN          NaN   NaN  303.052443  1117.186015  0.0   \n",
      "VRDeck        4197.0    NaN          NaN   NaN  310.710031  1246.994742  0.0   \n",
      "Name            4183   4176   Cints Erle     2         NaN          NaN  NaN   \n",
      "\n",
      "               25%   50%   75%      max  \n",
      "PassengerId    NaN   NaN   NaN      NaN  \n",
      "HomePlanet     NaN   NaN   NaN      NaN  \n",
      "CryoSleep      NaN   NaN   NaN      NaN  \n",
      "Cabin          NaN   NaN   NaN      NaN  \n",
      "Destination    NaN   NaN   NaN      NaN  \n",
      "Age           19.0  26.0  37.0     79.0  \n",
      "VIP            NaN   NaN   NaN      NaN  \n",
      "RoomService    0.0   0.0  53.0  11567.0  \n",
      "FoodCourt      0.0   0.0  78.0  25273.0  \n",
      "ShoppingMall   0.0   0.0  33.0   8292.0  \n",
      "Spa            0.0   0.0  50.0  19844.0  \n",
      "VRDeck         0.0   0.0  36.0  22272.0  \n",
      "Name           NaN   NaN   NaN      NaN   \n",
      " PassengerId     4277\n",
      "HomePlanet         3\n",
      "CryoSleep          2\n",
      "Cabin           3265\n",
      "Destination        3\n",
      "Age               79\n",
      "VIP                2\n",
      "RoomService      842\n",
      "FoodCourt        902\n",
      "ShoppingMall     715\n",
      "Spa              833\n",
      "VRDeck           796\n",
      "Name            4176\n",
      "dtype: int64 \n",
      " PassengerId       0\n",
      "HomePlanet       87\n",
      "CryoSleep        93\n",
      "Cabin           100\n",
      "Destination      92\n",
      "Age              91\n",
      "VIP              93\n",
      "RoomService      82\n",
      "FoodCourt       106\n",
      "ShoppingMall     98\n",
      "Spa             101\n",
      "VRDeck           80\n",
      "Name             94\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import test dataset into padas DataFrame & explore \n",
    "test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv') \n",
    "\n",
    "print(test.head(), '\\n', \n",
    "test.info(), '\\n', \n",
    "test.describe(include='all').T, '\\n', \n",
    "test.nunique() , '\\n', \n",
    "test.isna().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655e93e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:51.381262Z",
     "iopub.status.busy": "2023-08-06T17:05:51.380819Z",
     "iopub.status.idle": "2023-08-06T17:05:51.583214Z",
     "shell.execute_reply": "2023-08-06T17:05:51.581159Z"
    },
    "papermill": {
     "duration": 0.217302,
     "end_time": "2023-08-06T17:05:51.585777",
     "exception": false,
     "start_time": "2023-08-06T17:05:51.368475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12970 entries, 0 to 4276\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   12970 non-null  object \n",
      " 1   HomePlanet    12682 non-null  object \n",
      " 2   CryoSleep     12660 non-null  object \n",
      " 3   Cabin         12671 non-null  object \n",
      " 4   Destination   12696 non-null  object \n",
      " 5   Age           12700 non-null  float64\n",
      " 6   VIP           12674 non-null  object \n",
      " 7   RoomService   12707 non-null  float64\n",
      " 8   FoodCourt     12681 non-null  float64\n",
      " 9   ShoppingMall  12664 non-null  float64\n",
      " 10  Spa           12686 non-null  float64\n",
      " 11  VRDeck        12702 non-null  float64\n",
      " 12  Name          12676 non-null  object \n",
      " 13  Transported   8693 non-null   object \n",
      "dtypes: float64(6), object(8)\n",
      "memory usage: 1.5+ MB\n",
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
      "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
      "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
      "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
      "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "\n",
      "  Transported  \n",
      "0       False  \n",
      "1        True  \n",
      "2       False  \n",
      "3       False  \n",
      "4        True   \n",
      " None \n",
      "                 count unique               top   freq        mean  \\\n",
      "PassengerId     12970  12970           0001_01      1         NaN   \n",
      "HomePlanet      12682      3             Earth   6865         NaN   \n",
      "CryoSleep       12660      2             False   8079         NaN   \n",
      "Cabin           12671   9825           G/734/S      8         NaN   \n",
      "Destination     12696      3       TRAPPIST-1e   8871         NaN   \n",
      "Age           12700.0    NaN               NaN    NaN   28.771969   \n",
      "VIP             12674      2             False  12401         NaN   \n",
      "RoomService   12707.0    NaN               NaN    NaN  222.897852   \n",
      "FoodCourt     12681.0    NaN               NaN    NaN  451.961675   \n",
      "ShoppingMall  12664.0    NaN               NaN    NaN  174.906033   \n",
      "Spa           12686.0    NaN               NaN    NaN  308.476904   \n",
      "VRDeck        12702.0    NaN               NaN    NaN  306.789482   \n",
      "Name            12676  12629  Carry Contrevins      2         NaN   \n",
      "Transported      8693      2              True   4378         NaN   \n",
      "\n",
      "                      std  min   25%   50%   75%      max  \n",
      "PassengerId           NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "HomePlanet            NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "CryoSleep             NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Cabin                 NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Destination           NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Age             14.387261  0.0  19.0  27.0  38.0     79.0  \n",
      "VIP                   NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "RoomService    647.596664  0.0   0.0   0.0  49.0  14327.0  \n",
      "FoodCourt     1584.370747  0.0   0.0   0.0  77.0  29813.0  \n",
      "ShoppingMall    590.55869  0.0   0.0   0.0  29.0  23492.0  \n",
      "Spa           1130.279641  0.0   0.0   0.0  57.0  22408.0  \n",
      "VRDeck        1180.097223  0.0   0.0   0.0  42.0  24133.0  \n",
      "Name                  NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Transported           NaN  NaN   NaN   NaN   NaN      NaN   \n",
      " PassengerId     12970\n",
      "HomePlanet          3\n",
      "CryoSleep           2\n",
      "Cabin            9825\n",
      "Destination         3\n",
      "Age                80\n",
      "VIP                 2\n",
      "RoomService      1578\n",
      "FoodCourt        1953\n",
      "ShoppingMall     1367\n",
      "Spa              1679\n",
      "VRDeck           1642\n",
      "Name            12629\n",
      "Transported         2\n",
      "dtype: int64 \n",
      " PassengerId        0\n",
      "HomePlanet       288\n",
      "CryoSleep        310\n",
      "Cabin            299\n",
      "Destination      274\n",
      "Age              270\n",
      "VIP              296\n",
      "RoomService      263\n",
      "FoodCourt        289\n",
      "ShoppingMall     306\n",
      "Spa              284\n",
      "VRDeck           268\n",
      "Name             294\n",
      "Transported     4277\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# merge test and train datasets into single dataframe for data cleaning and necessary pre-processing \n",
    "all = pd.concat([train, test], axis=0) \n",
    "\n",
    "print(all.head(), '\\n', \n",
    "all.info(), '\\n', \n",
    "all.describe(include='all').T, '\\n', \n",
    "all.nunique() , '\\n', \n",
    "all.isna().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41fc2b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:51.611117Z",
     "iopub.status.busy": "2023-08-06T17:05:51.610030Z",
     "iopub.status.idle": "2023-08-06T17:05:52.015635Z",
     "shell.execute_reply": "2023-08-06T17:05:52.014583Z"
    },
    "papermill": {
     "duration": 0.421759,
     "end_time": "2023-08-06T17:05:52.018867",
     "exception": false,
     "start_time": "2023-08-06T17:05:51.597108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      object\n",
      "HomePlanet       object\n",
      "CryoSleep        object\n",
      "Cabin            object\n",
      "Destination      object\n",
      "Age             float64\n",
      "VIP              object\n",
      "RoomService     float64\n",
      "FoodCourt       float64\n",
      "ShoppingMall    float64\n",
      "Spa             float64\n",
      "VRDeck          float64\n",
      "Name             object\n",
      "Transported      object\n",
      "dtype: object\n",
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Name', 'Transported', 'group', 'id', 'deck', 'cabin_no', 'side',\n",
      "       'FName', 'LName'],\n",
      "      dtype='object')\n",
      "                count unique               top   freq        mean  \\\n",
      "PassengerId     12970  12970           0001_01      1         NaN   \n",
      "HomePlanet      12682      3             Earth   6865         NaN   \n",
      "CryoSleep       12660      2             False   8079         NaN   \n",
      "Cabin           12671   9825           G/734/S      8         NaN   \n",
      "Destination     12696      3       TRAPPIST-1e   8871         NaN   \n",
      "Age           12700.0    NaN               NaN    NaN   28.771969   \n",
      "VIP             12674      2             False  12401         NaN   \n",
      "RoomService   12707.0    NaN               NaN    NaN  222.897852   \n",
      "FoodCourt     12681.0    NaN               NaN    NaN  451.961675   \n",
      "ShoppingMall  12664.0    NaN               NaN    NaN  174.906033   \n",
      "Spa           12686.0    NaN               NaN    NaN  308.476904   \n",
      "VRDeck        12702.0    NaN               NaN    NaN  306.789482   \n",
      "Name            12676  12629  Carry Contrevins      2         NaN   \n",
      "Transported    8693.0    NaN               NaN    NaN    0.503624   \n",
      "group           12970   9280              6499      8         NaN   \n",
      "id              12970      8                01   9280         NaN   \n",
      "deck            12671      8                 F   4239         NaN   \n",
      "cabin_no        12671   1894                82     34         NaN   \n",
      "side            12671      2                 S   6381         NaN   \n",
      "FName           12676   2883             Luise     16         NaN   \n",
      "LName           12676   2406         Buckentry     19         NaN   \n",
      "\n",
      "                      std  min   25%   50%   75%      max  \n",
      "PassengerId           NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "HomePlanet            NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "CryoSleep             NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Cabin                 NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Destination           NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Age             14.387261  0.0  19.0  27.0  38.0     79.0  \n",
      "VIP                   NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "RoomService    647.596664  0.0   0.0   0.0  49.0  14327.0  \n",
      "FoodCourt     1584.370747  0.0   0.0   0.0  77.0  29813.0  \n",
      "ShoppingMall    590.55869  0.0   0.0   0.0  29.0  23492.0  \n",
      "Spa           1130.279641  0.0   0.0   0.0  57.0  22408.0  \n",
      "VRDeck        1180.097223  0.0   0.0   0.0  42.0  24133.0  \n",
      "Name                  NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "Transported      0.500016  0.0   0.0   1.0   1.0      1.0  \n",
      "group                 NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "id                    NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "deck                  NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "cabin_no              NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "side                  NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "FName                 NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "LName                 NaN  NaN   NaN   NaN   NaN      NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12970 entries, 0 to 4276\n",
      "Data columns (total 21 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   12970 non-null  object \n",
      " 1   HomePlanet    12682 non-null  object \n",
      " 2   CryoSleep     12660 non-null  object \n",
      " 3   Cabin         12671 non-null  object \n",
      " 4   Destination   12696 non-null  object \n",
      " 5   Age           12700 non-null  float64\n",
      " 6   VIP           12674 non-null  object \n",
      " 7   RoomService   12707 non-null  float64\n",
      " 8   FoodCourt     12681 non-null  float64\n",
      " 9   ShoppingMall  12664 non-null  float64\n",
      " 10  Spa           12686 non-null  float64\n",
      " 11  VRDeck        12702 non-null  float64\n",
      " 12  Name          12676 non-null  object \n",
      " 13  Transported   8693 non-null   float64\n",
      " 14  group         12970 non-null  int64  \n",
      " 15  id            12970 non-null  int64  \n",
      " 16  deck          12671 non-null  object \n",
      " 17  cabin_no      12671 non-null  float64\n",
      " 18  side          12671 non-null  object \n",
      " 19  FName         12676 non-null  object \n",
      " 20  LName         12676 non-null  object \n",
      "dtypes: float64(8), int64(2), object(11)\n",
      "memory usage: 2.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate rows \n",
    "all.drop_duplicates(keep='first', inplace=True) \n",
    "print(all.dtypes)\n",
    "\n",
    "# Feature Engineering \n",
    "# split PassengerId column into its component parts (group, and id ) as separate features  \n",
    "all[['group', 'id']] = all['PassengerId'].str.split(pat='_', expand=True) \n",
    "\n",
    "# split Cabin column into its component parts (deck, cabin id, side ) as separate features \n",
    "all[['deck', 'cabin_no', 'side']] = all.Cabin.str.split('/', expand=True) \n",
    "\n",
    "# split Name column into its component parts (first and last name) as separate features\n",
    "all[['FName', 'LName']] = all.Name.str.split(\" \", expand=True) \n",
    "''' The intuition behind splitting name is that family name or first name may potentially \n",
    "have some influence on who gets transported into the anomaly '''\n",
    "\n",
    "# Treatment of High-cardinality columns : columns that have too many categories (eg. upwards of 25 categories) \n",
    "''' add value_counts column for high cardinality columns & then drop original col, and treat resulting values as numerical data \n",
    "alternatively, transform high_cardinality column data into numbers (ordinal encoding) and treat as numerical data '''\n",
    "\n",
    "# drop unique value columns  \n",
    "print(all.columns)\n",
    "\n",
    "# Subsetting columns for pre-processing with relevant encoders \n",
    "''' Since most machine learning models will require all data in numerical form, we will use relevant \n",
    "encoders to transform our data accordingly. Numerical columns will not undergo any encoding. \n",
    "Columns with ordinal values (ranks, value_counts, high-cardinality columns, etc) will be processed \n",
    "using Ordinal Encoder. Columns with categorical values (low-cardinality unique un-ordered values) \n",
    "to be OneHotEncoded, but we will first convert them into numerical codes (with Ordinal Encoder) so \n",
    "the whole dataset is already numerical for missing value imputation using an Imputation algorithms, \n",
    "eg. KNNImputer we have used here. '''\n",
    "\n",
    "numeric = ['Age', 'group', 'id', 'cabin_no', 'RoomService', 'FoodCourt', 'ShoppingMall','Spa','VRDeck',] \n",
    "ordinal = ['FName', 'LName'] \n",
    "categorical = ['HomePlanet', 'Destination', 'deck', 'side', 'CryoSleep', 'VIP', 'Transported'] \n",
    "\n",
    "# checking unique values in each categorical columns to address \n",
    "# any mistakes (such as due to spelling, uppercase-lowercase or similar errors,)\n",
    "uniq_cats = {col: sorted(all[col].dropna().unique().tolist() ) for col in categorical} \n",
    "# print(uniq_cats) \n",
    "\n",
    "# Converting target column booleans to corresponding Integer values \n",
    "all.Transported = all.Transported.replace({True: 1, False:0})\n",
    "\n",
    "# Viewing summary statistics for all columns to check for any un-natural (eg. negative values where there should be none, etc) values,\n",
    "print(all.describe(include='all').T)\n",
    "\n",
    "# converting into numeric dtypes where numbers are appearing as object datatype \n",
    "all[['group', 'id', 'cabin_no']] = all[['group', 'id', 'cabin_no']].apply(pd.to_numeric, axis=0)\n",
    "print(all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7636c49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:52.044494Z",
     "iopub.status.busy": "2023-08-06T17:05:52.043629Z",
     "iopub.status.idle": "2023-08-06T17:05:52.115998Z",
     "shell.execute_reply": "2023-08-06T17:05:52.115015Z"
    },
    "papermill": {
     "duration": 0.089584,
     "end_time": "2023-08-06T17:05:52.120114",
     "exception": false,
     "start_time": "2023-08-06T17:05:52.030530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18\n",
      "Mean of target classes:  0.5036236051995858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Since this is a binary classification problem, calculating mean value of target column will suffice. \\nHere it is 0.503 indicating that the data very well balanced into positive and negative target classes. \\nIn case we had severely unbalanced data between target classes (say 0.75:0.25), we may have to address \\nthe imbalance, for example, by bootstrapping the minority class column to match the sample size of the \\nmajority class, or alternatively sampling from majority class column to match sample size of minority class'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping columns that have been split into their component values \n",
    "# setting original PassengerId column as row index \n",
    "all.set_index('PassengerId', inplace=True) \n",
    "\n",
    "# dropping Cabin & Name columns that were split previously \n",
    "all.drop(['Cabin', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# checking all columns in our dataset match with those covered under column groupings\n",
    "print(len(all.columns), len(numeric + ordinal + categorical) )\n",
    "\n",
    "# Transforming all NA value types (pandas, numpy NA types, etc) into np.nan type for uniformity \n",
    "all = all.fillna(np.nan) \n",
    "\n",
    "# Splitting our merged dataset into original train and test sets, \n",
    "# where original Test Set had missing ('NA') values in Transported column \n",
    "X_train = all[all.Transported.notna()].drop('Transported', axis=1)\n",
    "y_train = all[all.Transported.notna()].Transported.astype(int) \n",
    "X_test = all[all.Transported.isna()].drop('Transported', axis=1)\n",
    "\n",
    "# deleting the target column name from our categorical columns grouping list \n",
    "categorical.remove('Transported')\n",
    "\n",
    "# checking for data imbalance\n",
    "print('Mean of target classes: ', y_train.mean())\n",
    "\n",
    "''' Since this is a binary classification problem, calculating mean value of target column will suffice. \n",
    "Here it is 0.503 indicating that the data very well balanced into positive and negative target classes. \n",
    "In case we had severely unbalanced data between target classes (say 0.75:0.25), we may have to address \n",
    "the imbalance, for example, by bootstrapping the minority class column to match the sample size of the \n",
    "majority class, or alternatively sampling from majority class column to match sample size of minority class''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14597a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:52.146667Z",
     "iopub.status.busy": "2023-08-06T17:05:52.146015Z",
     "iopub.status.idle": "2023-08-06T17:05:52.398315Z",
     "shell.execute_reply": "2023-08-06T17:05:52.397147Z"
    },
    "papermill": {
     "duration": 0.26912,
     "end_time": "2023-08-06T17:05:52.401365",
     "exception": false,
     "start_time": "2023-08-06T17:05:52.132245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8693 entries, 0001_01 to 9280_02\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   HomePlanet    8492 non-null   float64\n",
      " 1   CryoSleep     8476 non-null   float64\n",
      " 2   Destination   8511 non-null   float64\n",
      " 3   Age           8514 non-null   float64\n",
      " 4   VIP           8490 non-null   float64\n",
      " 5   RoomService   8512 non-null   float64\n",
      " 6   FoodCourt     8510 non-null   float64\n",
      " 7   ShoppingMall  8485 non-null   float64\n",
      " 8   Spa           8510 non-null   float64\n",
      " 9   VRDeck        8505 non-null   float64\n",
      " 10  group         8693 non-null   int64  \n",
      " 11  id            8693 non-null   int64  \n",
      " 12  deck          8494 non-null   float64\n",
      " 13  cabin_no      8494 non-null   float64\n",
      " 14  side          8494 non-null   float64\n",
      " 15  FName         8493 non-null   float64\n",
      " 16  LName         8493 non-null   float64\n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 1.2+ MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 8693 entries, 0001_01 to 9280_02\n",
      "Series name: Transported\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "8693 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 135.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4277 entries, 0013_01 to 9277_01\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   HomePlanet    4190 non-null   float64\n",
      " 1   CryoSleep     4184 non-null   float64\n",
      " 2   Destination   4185 non-null   float64\n",
      " 3   Age           4186 non-null   float64\n",
      " 4   VIP           4184 non-null   float64\n",
      " 5   RoomService   4195 non-null   float64\n",
      " 6   FoodCourt     4171 non-null   float64\n",
      " 7   ShoppingMall  4179 non-null   float64\n",
      " 8   Spa           4176 non-null   float64\n",
      " 9   VRDeck        4197 non-null   float64\n",
      " 10  group         4277 non-null   int64  \n",
      " 11  id            4277 non-null   int64  \n",
      " 12  deck          4177 non-null   float64\n",
      " 13  cabin_no      4177 non-null   float64\n",
      " 14  side          4177 non-null   float64\n",
      " 15  FName         3896 non-null   float64\n",
      " 16  LName         3759 non-null   float64\n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 601.5+ KB\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "        HomePlanet    CryoSleep  Destination          Age          VIP  \\\n",
      "count  8492.000000  8476.000000  8511.000000  8514.000000  8490.000000   \n",
      "mean      0.665214     0.358306     1.483492    28.827930     0.023439   \n",
      "std       0.798155     0.479531     0.820237    14.489021     0.151303   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     1.000000    19.000000     0.000000   \n",
      "50%       0.000000     0.000000     2.000000    27.000000     0.000000   \n",
      "75%       1.000000     1.000000     2.000000    38.000000     0.000000   \n",
      "max       2.000000     1.000000     2.000000    79.000000     1.000000   \n",
      "\n",
      "        RoomService     FoodCourt  ShoppingMall           Spa        VRDeck  \\\n",
      "count   8512.000000   8510.000000   8485.000000   8510.000000   8505.000000   \n",
      "mean     224.687617    458.077203    173.729169    311.138778    304.854791   \n",
      "std      666.717663   1611.489240    604.696458   1136.705535   1145.717189   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       47.000000     76.000000     27.000000     59.000000     46.000000   \n",
      "max    14327.000000  29813.000000  23492.000000  22408.000000  24133.000000   \n",
      "\n",
      "             group           id         deck     cabin_no         side  \\\n",
      "count  8693.000000  8693.000000  8494.000000  8494.000000  8494.000000   \n",
      "mean   4633.389624     1.517773     4.305392   600.367671     0.504827   \n",
      "std    2671.028856     1.054241     1.778233   511.867226     0.500006   \n",
      "min       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
      "25%    2319.000000     1.000000     3.000000   167.250000     0.000000   \n",
      "50%    4630.000000     1.000000     5.000000   427.000000     1.000000   \n",
      "75%    6883.000000     2.000000     6.000000   999.000000     1.000000   \n",
      "max    9280.000000     8.000000     7.000000  1894.000000     1.000000   \n",
      "\n",
      "             FName        LName  \n",
      "count  8493.000000  8493.000000  \n",
      "mean   1344.921347  1110.891205  \n",
      "std     766.032078   641.481916  \n",
      "min       0.000000     0.000000  \n",
      "25%     710.000000   554.000000  \n",
      "50%    1346.000000  1110.000000  \n",
      "75%    1997.000000  1650.000000  \n",
      "max    2705.000000  2216.000000  \n",
      "\n",
      "count    8693.000000\n",
      "mean        0.503624\n",
      "std         0.500016\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: Transported, dtype: float64\n",
      "\n",
      "        HomePlanet    CryoSleep  Destination          Age          VIP  \\\n",
      "count  4190.000000  4184.000000  4185.000000  4186.000000  4184.000000   \n",
      "mean      0.680668     0.369025     1.505376    28.658146     0.017686   \n",
      "std       0.811815     0.482598     0.807489    14.179072     0.131825   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     1.000000    19.000000     0.000000   \n",
      "50%       0.000000     0.000000     2.000000    26.000000     0.000000   \n",
      "75%       1.000000     1.000000     2.000000    37.000000     0.000000   \n",
      "max       2.000000     1.000000     2.000000    79.000000     1.000000   \n",
      "\n",
      "        RoomService     FoodCourt  ShoppingMall           Spa        VRDeck  \\\n",
      "count   4195.000000   4171.000000   4179.000000   4176.000000   4197.000000   \n",
      "mean     219.266269    439.484296    177.295525    303.052443    310.710031   \n",
      "std      607.011289   1527.663045    560.821123   1117.186015   1246.994742   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       53.000000     78.000000     33.000000     50.000000     36.000000   \n",
      "max    11567.000000  25273.000000   8292.000000  19844.000000  22272.000000   \n",
      "\n",
      "             group           id         deck     cabin_no         side  \\\n",
      "count  4277.000000  4277.000000  4177.000000  4177.000000  4177.000000   \n",
      "mean   4639.296469     1.498714     4.353603   610.178836     0.501077   \n",
      "std    2716.197368     1.018221     1.719892   514.968131     0.500059   \n",
      "min      13.000000     1.000000     0.000000     0.000000     0.000000   \n",
      "25%    2249.000000     1.000000     3.000000   174.000000     0.000000   \n",
      "50%    4639.000000     1.000000     5.000000   442.000000     1.000000   \n",
      "75%    7030.000000     2.000000     6.000000  1027.000000     1.000000   \n",
      "max    9277.000000     8.000000     7.000000  1890.000000     1.000000   \n",
      "\n",
      "             FName        LName  \n",
      "count  3896.000000  3759.000000  \n",
      "mean   1337.273614  1069.688215  \n",
      "std     758.085440   636.404377  \n",
      "min       0.000000     0.000000  \n",
      "25%     707.750000   509.000000  \n",
      "50%    1342.500000  1061.000000  \n",
      "75%    1979.250000  1601.000000  \n",
      "max    2705.000000  2216.000000  \n"
     ]
    }
   ],
   "source": [
    "# Using encoders on column groupings to convert data into numerical values where applicable \n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan) \n",
    "\n",
    "''' we will fit the encoders to training data and transform both training and test datasets using the \n",
    "same patterns learned from training data. This is to avoid leakage (of insights from test data (which \n",
    "is not available at the time of training in real world application) into training dataset) '''\n",
    "X_train[ordinal + categorical] = ordinal_encoder.fit_transform(X_train[ordinal + categorical]) \n",
    "X_test[ordinal + categorical] = ordinal_encoder.transform(X_test[ordinal + categorical])\n",
    "\n",
    "# check dtypes for each column and correct where necessary \n",
    "print(X_train.info(), y_train.info(), X_test.info(), sep = '\\n\\n')\n",
    "print(X_train.describe(), y_train.describe(), X_test.describe(), sep = '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4163634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:52.428106Z",
     "iopub.status.busy": "2023-08-06T17:05:52.427653Z",
     "iopub.status.idle": "2023-08-06T17:05:52.471722Z",
     "shell.execute_reply": "2023-08-06T17:05:52.470397Z"
    },
    "papermill": {
     "duration": 0.060601,
     "end_time": "2023-08-06T17:05:52.474401",
     "exception": false,
     "start_time": "2023-08-06T17:05:52.413800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max spread of values in categorical columns: \n",
      " 0.0 7.0\n",
      "column-wise standard deviation in Training set after min-max scaling\n",
      "HomePlanet      0.798155\n",
      "CryoSleep       0.479531\n",
      "Destination     0.820237\n",
      "Age             1.283837\n",
      "VIP             0.151303\n",
      "RoomService     0.325750\n",
      "FoodCourt       0.378373\n",
      "ShoppingMall    0.180184\n",
      "Spa             0.355094\n",
      "VRDeck          0.332326\n",
      "group           2.015002\n",
      "id              1.054241\n",
      "deck            1.778233\n",
      "cabin_no        1.891801\n",
      "side            0.500006\n",
      "FName           1.982338\n",
      "LName           2.026342\n",
      "dtype: float64\n",
      "column-wise standard deviation in Test set after min-max scaling\n",
      "HomePlanet      0.811815\n",
      "CryoSleep       0.482598\n",
      "Destination     0.807489\n",
      "Age             1.256373\n",
      "VIP             0.131825\n",
      "RoomService     0.296578\n",
      "FoodCourt       0.358691\n",
      "ShoppingMall    0.167110\n",
      "Spa             0.348996\n",
      "VRDeck          0.361702\n",
      "group           2.049077\n",
      "id              1.018221\n",
      "deck            1.719892\n",
      "cabin_no        1.903261\n",
      "side            0.500059\n",
      "FName           1.961774\n",
      "LName           2.010303\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Scaling data before using missing value Imputation algorithm \n",
    "'''We are going to use KNN Imputer to impute missing values in our train and test datasets. \n",
    "Since KNN algorithm is informed by variances or deviation between datapoints, it is important that we first \n",
    "scale all data to a uniform scale. Now, since we want to later One-Hot-Encode the categorical \n",
    "columns which will require unique (integer) values, we cannot scale the categorical columns \n",
    "as that will turn them into float values. So, we will instead take the min and max values across\n",
    "all categorical clumns (calculates to 0 and 7 in this case) and scale the remaining data accordingly \n",
    "using MinMaxScaler '''\n",
    "\n",
    "# calculate min & max values for all categorical features combined \n",
    "min_cat = X_train[categorical].min().min()\n",
    "max_cat = X_train[categorical].max().max()\n",
    "print('Min-Max spread of values in categorical columns: \\n', min_cat, max_cat) \n",
    "\n",
    "# Scaling using MinMaxScaler \n",
    "mmScaler = MinMaxScaler(feature_range=(min_cat, max_cat)) \n",
    "X_train[numeric + ordinal] = mmScaler.fit_transform(X_train[numeric + ordinal]) \n",
    "X_test[numeric + ordinal] = mmScaler.transform(X_test[numeric + ordinal]) \n",
    "\n",
    "# checking column-wise standard deviation for scaled data \n",
    "print('column-wise standard deviation in Training set after min-max scaling')\n",
    "print(X_train.std(axis=0).T)\n",
    "print('column-wise standard deviation in Test set after min-max scaling')\n",
    "print(X_test.std(axis=0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd25b6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:52.501592Z",
     "iopub.status.busy": "2023-08-06T17:05:52.501204Z",
     "iopub.status.idle": "2023-08-06T17:05:55.054938Z",
     "shell.execute_reply": "2023-08-06T17:05:55.053869Z"
    },
    "papermill": {
     "duration": 2.57077,
     "end_time": "2023-08-06T17:05:55.057837",
     "exception": false,
     "start_time": "2023-08-06T17:05:52.487067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using KNN Imputer to impute missing values \n",
    "\n",
    "# ensure all NA values are represented uniformluy as np.nan \n",
    "X_train = X_train.fillna(np.nan)\n",
    "X_test = X_test.fillna(np.nan) \n",
    "\n",
    "# Instantiate KNNImputer using default 5 neighbors \n",
    "imputer = KNNImputer(n_neighbors=5) \n",
    "\n",
    "'''KNN Imputer will return numpy arrays. In order to retain our dataframe with column names \n",
    "(to use for further preprocessig), we will assign the numpy output to our pandas dataframes \n",
    "using .loc[:] accessor for all data. '''\n",
    "\n",
    "X_train.loc[:] = imputer.fit_transform(X_train) \n",
    "X_test.loc[:] = imputer.transform(X_test) \n",
    "\n",
    "# Checking if any missing values still remain \n",
    "# print(X_train.isna().sum().T) \n",
    "# print(X_test.isna().sum().T) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37729947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:55.084942Z",
     "iopub.status.busy": "2023-08-06T17:05:55.084542Z",
     "iopub.status.idle": "2023-08-06T17:05:56.321861Z",
     "shell.execute_reply": "2023-08-06T17:05:56.319780Z"
    },
    "papermill": {
     "duration": 1.254122,
     "end_time": "2023-08-06T17:05:56.324571",
     "exception": false,
     "start_time": "2023-08-06T17:05:55.070449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c2af3_row0_col0, #T_c2af3_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row1_col0 {\n",
       "  background-color: #6ba5cd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row1_col1 {\n",
       "  background-color: #569dc8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row2_col0 {\n",
       "  background-color: #83afd3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row2_col1 {\n",
       "  background-color: #73a9cf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row3_col0 {\n",
       "  background-color: #8eb3d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row3_col1 {\n",
       "  background-color: #358fc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c2af3_row4_col0 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row4_col1 {\n",
       "  background-color: #e5e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row5_col0 {\n",
       "  background-color: #d2d3e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row5_col1 {\n",
       "  background-color: #ede7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row6_col0 {\n",
       "  background-color: #d7d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row6_col1 {\n",
       "  background-color: #faf2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row7_col0 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row7_col1 {\n",
       "  background-color: #fef6fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row8_col0 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row8_col1 {\n",
       "  background-color: #f1ebf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row9_col0 {\n",
       "  background-color: #ece7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row9_col1, #T_c2af3_row14_col0 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row10_col0, #T_c2af3_row11_col0 {\n",
       "  background-color: #f2ecf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row10_col1, #T_c2af3_row12_col1, #T_c2af3_row15_col0, #T_c2af3_row16_col0 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row11_col1 {\n",
       "  background-color: #cdd0e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row12_col0 {\n",
       "  background-color: #f5eff6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row13_col0 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row13_col1 {\n",
       "  background-color: #ede8f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row14_col1 {\n",
       "  background-color: #dedcec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row15_col1 {\n",
       "  background-color: #dddbec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c2af3_row16_col1 {\n",
       "  background-color: #94b6d7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c2af3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c2af3_level0_col0\" class=\"col_heading level0 col0\" >correlation</th>\n",
       "      <th id=\"T_c2af3_level0_col1\" class=\"col_heading level0 col1\" >mutual_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row0\" class=\"row_heading level0 row0\" >CryoSleep</th>\n",
       "      <td id=\"T_c2af3_row0_col0\" class=\"data row0 col0\" >0.464822</td>\n",
       "      <td id=\"T_c2af3_row0_col1\" class=\"data row0 col1\" >0.110660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row1\" class=\"row_heading level0 row1\" >RoomService</th>\n",
       "      <td id=\"T_c2af3_row1_col0\" class=\"data row1 col0\" >0.243517</td>\n",
       "      <td id=\"T_c2af3_row1_col1\" class=\"data row1 col1\" >0.065398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row2\" class=\"row_heading level0 row2\" >Spa</th>\n",
       "      <td id=\"T_c2af3_row2_col0\" class=\"data row2 col0\" >0.218882</td>\n",
       "      <td id=\"T_c2af3_row2_col1\" class=\"data row2 col1\" >0.059129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row3\" class=\"row_heading level0 row3\" >VRDeck</th>\n",
       "      <td id=\"T_c2af3_row3_col0\" class=\"data row3 col0\" >0.206177</td>\n",
       "      <td id=\"T_c2af3_row3_col1\" class=\"data row3 col1\" >0.072151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row4\" class=\"row_heading level0 row4\" >HomePlanet</th>\n",
       "      <td id=\"T_c2af3_row4_col0\" class=\"data row4 col0\" >0.119572</td>\n",
       "      <td id=\"T_c2af3_row4_col1\" class=\"data row4 col1\" >0.023737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row5\" class=\"row_heading level0 row5\" >deck</th>\n",
       "      <td id=\"T_c2af3_row5_col0\" class=\"data row5 col0\" >0.117175</td>\n",
       "      <td id=\"T_c2af3_row5_col1\" class=\"data row5 col1\" >0.020142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row6\" class=\"row_heading level0 row6\" >Destination</th>\n",
       "      <td id=\"T_c2af3_row6_col0\" class=\"data row6 col0\" >0.109219</td>\n",
       "      <td id=\"T_c2af3_row6_col1\" class=\"data row6 col1\" >0.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row7\" class=\"row_heading level0 row7\" >side</th>\n",
       "      <td id=\"T_c2af3_row7_col0\" class=\"data row7 col0\" >0.103062</td>\n",
       "      <td id=\"T_c2af3_row7_col1\" class=\"data row7 col1\" >0.008276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row8\" class=\"row_heading level0 row8\" >Age</th>\n",
       "      <td id=\"T_c2af3_row8_col0\" class=\"data row8 col0\" >0.072878</td>\n",
       "      <td id=\"T_c2af3_row8_col1\" class=\"data row8 col1\" >0.017392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row9\" class=\"row_heading level0 row9\" >id</th>\n",
       "      <td id=\"T_c2af3_row9_col0\" class=\"data row9 col0\" >0.066390</td>\n",
       "      <td id=\"T_c2af3_row9_col1\" class=\"data row9 col1\" >0.009302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row10\" class=\"row_heading level0 row10\" >cabin_no</th>\n",
       "      <td id=\"T_c2af3_row10_col0\" class=\"data row10 col0\" >0.046197</td>\n",
       "      <td id=\"T_c2af3_row10_col1\" class=\"data row10 col1\" >0.007547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row11\" class=\"row_heading level0 row11\" >FoodCourt</th>\n",
       "      <td id=\"T_c2af3_row11_col0\" class=\"data row11 col0\" >0.046167</td>\n",
       "      <td id=\"T_c2af3_row11_col1\" class=\"data row11 col1\" >0.034406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row12\" class=\"row_heading level0 row12\" >VIP</th>\n",
       "      <td id=\"T_c2af3_row12_col0\" class=\"data row12 col0\" >0.036606</td>\n",
       "      <td id=\"T_c2af3_row12_col1\" class=\"data row12 col1\" >0.007585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row13\" class=\"row_heading level0 row13\" >group</th>\n",
       "      <td id=\"T_c2af3_row13_col0\" class=\"data row13 col0\" >0.021491</td>\n",
       "      <td id=\"T_c2af3_row13_col1\" class=\"data row13 col1\" >0.019838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row14\" class=\"row_heading level0 row14\" >LName</th>\n",
       "      <td id=\"T_c2af3_row14_col0\" class=\"data row14 col0\" >0.015498</td>\n",
       "      <td id=\"T_c2af3_row14_col1\" class=\"data row14 col1\" >0.026962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row15\" class=\"row_heading level0 row15\" >FName</th>\n",
       "      <td id=\"T_c2af3_row15_col0\" class=\"data row15 col0\" >0.009261</td>\n",
       "      <td id=\"T_c2af3_row15_col1\" class=\"data row15 col1\" >0.027551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c2af3_level0_row16\" class=\"row_heading level0 row16\" >ShoppingMall</th>\n",
       "      <td id=\"T_c2af3_row16_col0\" class=\"data row16 col0\" >0.007738</td>\n",
       "      <td id=\"T_c2af3_row16_col1\" class=\"data row16 col1\" >0.050799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7b4320654a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics after missing value imputation: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8693 entries, 0001_01 to 9280_02\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   HomePlanet    8693 non-null   float64\n",
      " 1   CryoSleep     8693 non-null   float64\n",
      " 2   Destination   8693 non-null   float64\n",
      " 3   Age           8693 non-null   float64\n",
      " 4   VIP           8693 non-null   float64\n",
      " 5   RoomService   8693 non-null   float64\n",
      " 6   FoodCourt     8693 non-null   float64\n",
      " 7   ShoppingMall  8693 non-null   float64\n",
      " 8   Spa           8693 non-null   float64\n",
      " 9   VRDeck        8693 non-null   float64\n",
      " 10  group         8693 non-null   float64\n",
      " 11  id            8693 non-null   float64\n",
      " 12  deck          8693 non-null   float64\n",
      " 13  cabin_no      8693 non-null   float64\n",
      " 14  side          8693 non-null   float64\n",
      " 15  FName         8693 non-null   float64\n",
      " 16  LName         8693 non-null   float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "               count      mean       std  min       25%       50%       75%  \\\n",
      "HomePlanet    8693.0  0.663407  0.793190  0.0  0.000000  0.000000  1.000000   \n",
      "CryoSleep     8693.0  0.359117  0.475694  0.0  0.000000  0.000000  1.000000   \n",
      "Destination   8693.0  1.484988  0.813339  0.0  1.000000  2.000000  2.000000   \n",
      "Age           8693.0  2.554257  1.274142  0.0  1.683544  2.392405  3.367089   \n",
      "VIP           8693.0  0.023214  0.149795  0.0  0.000000  0.000000  0.000000   \n",
      "RoomService   8693.0  0.109874  0.323254  0.0  0.000000  0.000000  0.028338   \n",
      "FoodCourt     8693.0  0.107324  0.375431  0.0  0.000000  0.000000  0.020662   \n",
      "ShoppingMall  8693.0  0.051620  0.178397  0.0  0.000000  0.000000  0.009535   \n",
      "Spa           8693.0  0.097355  0.352030  0.0  0.000000  0.000000  0.022804   \n",
      "VRDeck        8693.0  0.088063  0.329733  0.0  0.000000  0.000000  0.015373   \n",
      "group         8693.0  3.494636  2.015002  0.0  1.748680  3.492079  5.191723   \n",
      "id            8693.0  0.517773  1.054241  0.0  0.000000  0.000000  1.000000   \n",
      "deck          8693.0  4.306476  1.770726  0.0  3.000000  5.000000  6.000000   \n",
      "cabin_no      8693.0  2.213791  1.883950  0.0  0.620908  1.581837  3.673706   \n",
      "side          8693.0  0.505326  0.495359  0.0  0.000000  0.600000  1.000000   \n",
      "FName         8693.0  3.482787  1.964260  0.0  1.863216  3.490943  5.123845   \n",
      "LName         8693.0  3.507923  2.007687  0.0  1.787906  3.506318  5.180505   \n",
      "\n",
      "              max  \n",
      "HomePlanet    2.0  \n",
      "CryoSleep     1.0  \n",
      "Destination   2.0  \n",
      "Age           7.0  \n",
      "VIP           1.0  \n",
      "RoomService   7.0  \n",
      "FoodCourt     7.0  \n",
      "ShoppingMall  7.0  \n",
      "Spa           7.0  \n",
      "VRDeck        7.0  \n",
      "group         7.0  \n",
      "id            7.0  \n",
      "deck          7.0  \n",
      "cabin_no      7.0  \n",
      "side          1.0  \n",
      "FName         7.0  \n",
      "LName         7.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4277 entries, 0013_01 to 9277_01\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   HomePlanet    4277 non-null   float64\n",
      " 1   CryoSleep     4277 non-null   float64\n",
      " 2   Destination   4277 non-null   float64\n",
      " 3   Age           4277 non-null   float64\n",
      " 4   VIP           4277 non-null   float64\n",
      " 5   RoomService   4277 non-null   float64\n",
      " 6   FoodCourt     4277 non-null   float64\n",
      " 7   ShoppingMall  4277 non-null   float64\n",
      " 8   Spa           4277 non-null   float64\n",
      " 9   VRDeck        4277 non-null   float64\n",
      " 10  group         4277 non-null   float64\n",
      " 11  id            4277 non-null   float64\n",
      " 12  deck          4277 non-null   float64\n",
      " 13  cabin_no      4277 non-null   float64\n",
      " 14  side          4277 non-null   float64\n",
      " 15  FName         4277 non-null   float64\n",
      " 16  LName         4277 non-null   float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 601.5+ KB\n",
      "None\n",
      "               count      mean       std       min       25%       50%  \\\n",
      "HomePlanet    4277.0  0.679448  0.806186  0.000000  0.000000  0.000000   \n",
      "CryoSleep     4277.0  0.369698  0.479142  0.000000  0.000000  0.000000   \n",
      "Destination   4277.0  1.506710  0.801334  0.000000  1.000000  2.000000   \n",
      "Age           4277.0  2.541217  1.246635  0.000000  1.772152  2.303797   \n",
      "VIP           4277.0  0.017816  0.130805  0.000000  0.000000  0.000000   \n",
      "RoomService   4277.0  0.106932  0.294393  0.000000  0.000000  0.000000   \n",
      "FoodCourt     4277.0  0.102653  0.355811  0.000000  0.000000  0.000000   \n",
      "ShoppingMall  4277.0  0.053220  0.166964  0.000000  0.000000  0.000000   \n",
      "Spa           4277.0  0.094317  0.345609  0.000000  0.000000  0.000000   \n",
      "VRDeck        4277.0  0.089385  0.358569  0.000000  0.000000  0.000000   \n",
      "group         4277.0  3.499092  2.049077  0.009053  1.695872  3.498868   \n",
      "id            4277.0  0.498714  1.018221  0.000000  0.000000  0.000000   \n",
      "deck          4277.0  4.360393  1.710030  0.000000  3.000000  5.000000   \n",
      "cabin_no      4277.0  2.266319  1.898600  0.000000  0.654171  1.655755   \n",
      "side          4277.0  0.500351  0.494982  0.000000  0.000000  0.600000   \n",
      "FName         4277.0  3.467896  1.891856  0.000000  1.922736  3.488355   \n",
      "LName         4277.0  3.399559  1.910540  0.000000  1.825812  3.411552   \n",
      "\n",
      "                   75%       max  \n",
      "HomePlanet    1.000000  2.000000  \n",
      "CryoSleep     1.000000  1.000000  \n",
      "Destination   2.000000  2.000000  \n",
      "Age           3.278481  7.000000  \n",
      "VIP           0.000000  1.000000  \n",
      "RoomService   0.029804  5.651497  \n",
      "FoodCourt     0.021132  5.934022  \n",
      "ShoppingMall  0.011919  2.470799  \n",
      "Spa           0.019368  6.199036  \n",
      "VRDeck        0.011892  6.460200  \n",
      "group         5.302619  6.997737  \n",
      "id            1.000000  7.000000  \n",
      "deck          6.000000  7.000000  \n",
      "cabin_no      3.803062  6.985216  \n",
      "side          1.000000  1.000000  \n",
      "FName         4.971165  7.000000  \n",
      "LName         4.880415  7.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculating correlation (linear) and mutual-info (non-linear) of dataset columns / features with target column \n",
    "correlation = pd.concat([X_train, y_train], axis=1).corr().Transported.abs().values[:-1]\n",
    "mutual_info = mutual_info_regression(X_train, y_train, random_state=SEED) \n",
    "associations = {'correlation': correlation, 'mutual_info': mutual_info} \n",
    "associations = pd.DataFrame(associations, index=X_train.columns).sort_values(by='correlation', ascending=False)\n",
    "display(associations.style.background_gradient() )\n",
    "\n",
    "''' Instead of drawing multiple visualizations to see how various features interact with \n",
    "each other and with the target variable, I prefer to use a correlation / mutual-info matrix that gives me \n",
    "a snapshot of interactions in one tabular view. Looking at the results of both linear correlation\n",
    "and non-linear mutual-info regression, it appears Cryosleep status had the maximum influence \n",
    "on who was Transported into the anomaly'''\n",
    "\n",
    "# Checking summary statistics after missing value imputation \n",
    "print('Summary statistics after missing value imputation: ') \n",
    "print(X_train.info(), X_train.describe().T, sep='\\n') \n",
    "print(X_test.info(), X_test.describe().T, sep='\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d459f7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.353187Z",
     "iopub.status.busy": "2023-08-06T17:05:56.352773Z",
     "iopub.status.idle": "2023-08-06T17:05:56.454156Z",
     "shell.execute_reply": "2023-08-06T17:05:56.453022Z"
    },
    "papermill": {
     "duration": 0.119036,
     "end_time": "2023-08-06T17:05:56.457139",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.338103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OHE\n",
      "(8693, 17)\n",
      "(4277, 17)\n",
      "After OHE\n",
      "(8693, 69)\n",
      "(4277, 69)\n",
      "[6954, 1739, 6954, 1739, 6954, 1739]\n",
      "SEED:  66451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Transforming Categorical features using OneHotEncoder \n",
    "print('Before OHE', X_train.shape, X_test.shape, sep='\\n')\n",
    "onehot = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False) \n",
    "onehot = make_column_transformer((onehot, categorical), remainder='passthrough')\n",
    "X_train = onehot.fit_transform(X_train)\n",
    "X_test = onehot.transform(X_test) \n",
    "print('After OHE', X_train.shape, X_test.shape, sep='\\n')\n",
    "\n",
    "# # Dimensionality Reduction using PCA \n",
    "# pca = PCA(n_components=0.99, random_state=SEED) \n",
    "# X_train = pca.fit_transform(X_train) \n",
    "# X_test = pca.transform(X_test) \n",
    "# print('After PCA', X_train.shape, X_test.shape, sep='\\n')\n",
    "# ''' PCA is adversely affecting performance in all my trials with it, so excluding it ''' \n",
    "\n",
    "# scaler = StandardScaler(with_mean=True)\n",
    "scaler = RobustScaler(with_centering=True)\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test = scaler.transform(X_test) \n",
    "\n",
    "# Retaining full training set for training model to be used for submission  \n",
    "X_train_full, y_train_full = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Creating training and validation split (including corresponding indices from the base set) to evaluate model performances \n",
    "X_train, X_val, y_train, y_val, tts_tr, tts_val = train_test_split(X_train_full, y_train_full, np.arange(X_train_full.shape[0]), \n",
    "                                                                   test_size=0.20, stratify=y_train_full, random_state=SEED) \n",
    "print([i.shape[0] for i in [X_train, X_val, y_train, y_val, tts_tr, tts_val] ]) \n",
    "print('SEED: ', SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5242137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.487160Z",
     "iopub.status.busy": "2023-08-06T17:05:56.486691Z",
     "iopub.status.idle": "2023-08-06T17:05:56.496241Z",
     "shell.execute_reply": "2023-08-06T17:05:56.495055Z"
    },
    "papermill": {
     "duration": 0.02793,
     "end_time": "2023-08-06T17:05:56.498649",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.470719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" TPOT returned various tuned versions of RandomForestClassifier, none of which gave a competition \\naccuuracy score of more than 77% (much less than the score achieved by other models in this Notebook) \\nso I didn't pursue it any further. \\n\\nFor reference, some of the pipelines & model returned by various TPOT runs (model parameters with their \\ncorresponding neg_log_loss score) are as follows:\\n\\nBest pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.5, min_samples_leaf=17, min_samples_split=12, n_estimators=100)\\n-0.312272328007534 \\n\\nBest pipeline: RandomForestClassifier(CombineDFs(Normalizer(input_matrix, norm=max), input_matrix), bootstrap=True, criterion=entropy, max_features=0.9500000000000001, min_samples_leaf=15, min_samples_split=7, n_estimators=100)\\n-0.3149593362816865\\n\\nBest pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.6500000000000001, min_samples_leaf=8, min_samples_split=19, n_estimators=100)\\n-0.29370608855794433 \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # exploring pipelines, including algorithms, using TPOT library \n",
    "# from tpot import TPOTClassifier \n",
    "# tpot = TPOTClassifier(generations=None, population_size=7, verbosity=2, offspring_size=10, \n",
    "#                       scoring='neg_log_loss', cv=5, n_jobs=-1, max_time_mins=10, ) \n",
    "# tpot.fit(X_train, y_train) \n",
    "# print(tpot.score(X_train, y_train))\n",
    "\n",
    "''' TPOT returned various tuned versions of RandomForestClassifier, none of which gave a competition \n",
    "accuuracy score of more than 77% (much less than the score achieved by other models in this Notebook) \n",
    "so I didn't pursue it any further. \n",
    "\n",
    "For reference, some of the pipelines & model returned by various TPOT runs (model parameters with their \n",
    "corresponding neg_log_loss score) are as follows:\n",
    "\n",
    "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.5, min_samples_leaf=17, min_samples_split=12, n_estimators=100)\n",
    "-0.312272328007534 \n",
    "\n",
    "Best pipeline: RandomForestClassifier(CombineDFs(Normalizer(input_matrix, norm=max), input_matrix), bootstrap=True, criterion=entropy, max_features=0.9500000000000001, min_samples_leaf=15, min_samples_split=7, n_estimators=100)\n",
    "-0.3149593362816865\n",
    "\n",
    "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.6500000000000001, min_samples_leaf=8, min_samples_split=19, n_estimators=100)\n",
    "-0.29370608855794433 ''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "985a01d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.528566Z",
     "iopub.status.busy": "2023-08-06T17:05:56.527627Z",
     "iopub.status.idle": "2023-08-06T17:05:56.534449Z",
     "shell.execute_reply": "2023-08-06T17:05:56.533560Z"
    },
    "papermill": {
     "duration": 0.024261,
     "end_time": "2023-08-06T17:05:56.536703",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.512442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Modeling - Running various model configurations to find the model that gives the best log_loss score\n",
    "# # I have not included keras neural networks here, because we will construct and compare those separately later \n",
    "\n",
    "# rs = dict(random_state=SEED)\n",
    "# models = []\n",
    "# models += [LogisticRegression(max_iter=200), SVC(probability=True, **rs), KNeighborsClassifier(), DecisionTreeClassifier(**rs), \n",
    "#            RandomForestClassifier(**rs), AdaBoostClassifier(**rs), GradientBoostingClassifier(**rs), MLPClassifier(max_iter=200, **rs), \n",
    "#            GaussianNB()] \n",
    "# models += [GradientBoostingClassifier(learning_rate=0.05, max_depth=7, max_features='log2', n_estimators=154, subsample=0.55, **rs), \n",
    "#            GradientBoostingClassifier(subsample=0.9, **rs), ]\n",
    "# models += [RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=0.5, min_samples_leaf=17, min_samples_split=12, n_estimators=100, **rs), \n",
    "#            RandomForestClassifier(bootstrap=True, criterion='entropy', max_features=0.65, min_samples_leaf=8, min_samples_split=19, n_estimators=100, **rs),\n",
    "#            RandomForestClassifier(bootstrap=True, criterion='entropy', max_features=0.95, min_samples_leaf=15, min_samples_split=7, n_estimators=100, **rs)]\n",
    "# models += [GradientBoostingClassifier(max_depth=7, max_features='sqrt', n_estimators=600, n_iter_no_change=50, tol=0, subsample=0.75, learning_rate=0.05, validation_fraction=0.2, verbose=0, random_state=SEED)]\n",
    "# models += [XGBClassifier(**rs), LGBMClassifier(**rs), \n",
    "#            LGBMClassifier(**rs, n_estimators=600, learning_rate=0.05, metric='binary_logloss',) ]\n",
    "\n",
    "# # Creating empty lists to record various model scores for comparison \n",
    "# logloss, accu, roc, mdl = [], [], [], [] \n",
    "\n",
    "# ''' Since this is a binary classification problem, I have relied on \n",
    "# log_loss / binary_crossentropy scores to compare model performances'''\n",
    "\n",
    "# # X_val, y_val = X_train_full, y_train_full # to check score on full training set - use only when required \n",
    "\n",
    "# # fitting models on training set & recording prediction scores on validation set \n",
    "# for model in models:\n",
    "#     if model.__class__.__name__ == 'LGBMClassifier': \n",
    "#         model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_val, y_val)], verbose=0 ) \n",
    "#     else: \n",
    "#         model.fit(X_train, y_train) \n",
    "#     y_predprob_val = model.predict_proba(X_val)[:, 1] \n",
    "#     accu.append(model.score(X_val, y_val))\n",
    "#     logloss.append(log_loss(y_val, y_predprob_val) )\n",
    "#     roc.append(roc_auc_score(y_val, y_predprob_val))\n",
    "#     mdl.append(model.__class__.__name__) \n",
    "#     print('.', end='')\n",
    "\n",
    "# # saving scores in a dataframe for convenient viewing and analysis \n",
    "# scores_df = pd.DataFrame(dict(mdl=mdl, logloss=logloss, accuracy=accu, roc_score=roc) ) \n",
    "# print('\\n', scores_df.sort_values('logloss')) \n",
    "# print('\\n-Above list sorted by logloss, below is in the order of model fitting-')\n",
    "# print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da00d4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.565956Z",
     "iopub.status.busy": "2023-08-06T17:05:56.565551Z",
     "iopub.status.idle": "2023-08-06T17:05:56.576603Z",
     "shell.execute_reply": "2023-08-06T17:05:56.575345Z"
    },
    "papermill": {
     "duration": 0.028808,
     "end_time": "2023-08-06T17:05:56.579184",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.550376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the observations in our above trials, although LGBMClassifier shows similar metrics \\nas GradientBoostingClassifier, in the actual submissions GradientBoostingClassifier has consistently \\noutperformed other models, so I have tried to improve this algorithm via hyperparameters-tuning. We will \\ncontinue adding more models to our lists of models to compare performance as we keep improving our models \\n\\nFollowing results obtained from above code (using SEED=4326) for reference\\n                           mdl  logloss  accuracy  roc_score\\n17              LGBMClassifier     0.39      0.81       0.90\\n16              LGBMClassifier     0.39      0.81       0.90\\n14  GradientBoostingClassifier     0.40      0.81       0.90\\n10  GradientBoostingClassifier     0.40      0.81       0.90\\n6   GradientBoostingClassifier     0.40      0.81       0.90\\n9   GradientBoostingClassifier     0.40      0.81       0.90\\n11      RandomForestClassifier     0.41      0.81       0.89\\n13      RandomForestClassifier     0.42      0.80       0.89\\n15               XGBClassifier     0.42      0.81       0.89\\n12      RandomForestClassifier     0.43      0.81       0.89\\n4       RandomForestClassifier     0.44      0.81       0.89\\n7                MLPClassifier     0.44      0.78       0.88\\n0           LogisticRegression     0.45      0.79       0.87\\n1                          SVC     0.50      0.79       0.86\\n5           AdaBoostClassifier     0.67      0.80       0.88\\n2         KNeighborsClassifier     2.10      0.77       0.84\\n8                   GaussianNB     5.03      0.53       0.82\\n3       DecisionTreeClassifier     9.49      0.74       0.74\\n\\n-Above list sorted by logloss, below is in the order of model fitting-\\n                           mdl  logloss  accuracy  roc_score\\n0           LogisticRegression     0.45      0.79       0.87\\n1                          SVC     0.50      0.79       0.86\\n2         KNeighborsClassifier     2.10      0.77       0.84\\n3       DecisionTreeClassifier     9.49      0.74       0.74\\n4       RandomForestClassifier     0.44      0.81       0.89\\n5           AdaBoostClassifier     0.67      0.80       0.88\\n6   GradientBoostingClassifier     0.40      0.81       0.90\\n7                MLPClassifier     0.44      0.78       0.88\\n8                   GaussianNB     5.03      0.53       0.82\\n9   GradientBoostingClassifier     0.40      0.81       0.90\\n10  GradientBoostingClassifier     0.40      0.81       0.90\\n11      RandomForestClassifier     0.41      0.81       0.89\\n12      RandomForestClassifier     0.43      0.81       0.89\\n13      RandomForestClassifier     0.42      0.80       0.89\\n14  GradientBoostingClassifier     0.40      0.81       0.90\\n15               XGBClassifier     0.42      0.81       0.89\\n16              LGBMClassifier     0.39      0.81       0.90\\n17              LGBMClassifier     0.39      0.81       0.90\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Based on the observations in our above trials, although LGBMClassifier shows similar metrics \n",
    "as GradientBoostingClassifier, in the actual submissions GradientBoostingClassifier has consistently \n",
    "outperformed other models, so I have tried to improve this algorithm via hyperparameters-tuning. We will \n",
    "continue adding more models to our lists of models to compare performance as we keep improving our models \n",
    "\n",
    "Following results obtained from above code (using SEED=4326) for reference\n",
    "                           mdl  logloss  accuracy  roc_score\n",
    "17              LGBMClassifier     0.39      0.81       0.90\n",
    "16              LGBMClassifier     0.39      0.81       0.90\n",
    "14  GradientBoostingClassifier     0.40      0.81       0.90\n",
    "10  GradientBoostingClassifier     0.40      0.81       0.90\n",
    "6   GradientBoostingClassifier     0.40      0.81       0.90\n",
    "9   GradientBoostingClassifier     0.40      0.81       0.90\n",
    "11      RandomForestClassifier     0.41      0.81       0.89\n",
    "13      RandomForestClassifier     0.42      0.80       0.89\n",
    "15               XGBClassifier     0.42      0.81       0.89\n",
    "12      RandomForestClassifier     0.43      0.81       0.89\n",
    "4       RandomForestClassifier     0.44      0.81       0.89\n",
    "7                MLPClassifier     0.44      0.78       0.88\n",
    "0           LogisticRegression     0.45      0.79       0.87\n",
    "1                          SVC     0.50      0.79       0.86\n",
    "5           AdaBoostClassifier     0.67      0.80       0.88\n",
    "2         KNeighborsClassifier     2.10      0.77       0.84\n",
    "8                   GaussianNB     5.03      0.53       0.82\n",
    "3       DecisionTreeClassifier     9.49      0.74       0.74\n",
    "\n",
    "-Above list sorted by logloss, below is in the order of model fitting-\n",
    "                           mdl  logloss  accuracy  roc_score\n",
    "0           LogisticRegression     0.45      0.79       0.87\n",
    "1                          SVC     0.50      0.79       0.86\n",
    "2         KNeighborsClassifier     2.10      0.77       0.84\n",
    "3       DecisionTreeClassifier     9.49      0.74       0.74\n",
    "4       RandomForestClassifier     0.44      0.81       0.89\n",
    "5           AdaBoostClassifier     0.67      0.80       0.88\n",
    "6   GradientBoostingClassifier     0.40      0.81       0.90\n",
    "7                MLPClassifier     0.44      0.78       0.88\n",
    "8                   GaussianNB     5.03      0.53       0.82\n",
    "9   GradientBoostingClassifier     0.40      0.81       0.90\n",
    "10  GradientBoostingClassifier     0.40      0.81       0.90\n",
    "11      RandomForestClassifier     0.41      0.81       0.89\n",
    "12      RandomForestClassifier     0.43      0.81       0.89\n",
    "13      RandomForestClassifier     0.42      0.80       0.89\n",
    "14  GradientBoostingClassifier     0.40      0.81       0.90\n",
    "15               XGBClassifier     0.42      0.81       0.89\n",
    "16              LGBMClassifier     0.39      0.81       0.90\n",
    "17              LGBMClassifier     0.39      0.81       0.90\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902a10a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.609343Z",
     "iopub.status.busy": "2023-08-06T17:05:56.608883Z",
     "iopub.status.idle": "2023-08-06T17:05:56.615173Z",
     "shell.execute_reply": "2023-08-06T17:05:56.613910Z"
    },
    "papermill": {
     "duration": 0.024318,
     "end_time": "2023-08-06T17:05:56.617755",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.593437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters-tuning the best performing model so far - GradientBoostingClassifier \n",
    "# models = [GradientBoostingClassifier(n_estimators=300, n_iter_no_change=25, tol=0.0001,)  ] \n",
    "# ftd_model = {}\n",
    "# gb_params = {\"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3], \"subsample\": np.arange(0.40, 1.05, 0.05), \n",
    "#              \"max_depth\": np.arange(4, 10), # \"n_estimators\": np.arange(100, 300), \n",
    "#              \"max_features\": ['sqrt', 'log2', None], \"validation_fraction\": [0.25], } \n",
    "# cv = [(tts_tr, tts_val)]\n",
    "# for model in models:\n",
    "#     rcv = RandomizedSearchCV(estimator=model, param_distributions=gb_params, n_iter=72, n_jobs=-1, \n",
    "#                              random_state=SEED, scoring='neg_log_loss', cv=cv, verbose=3, ) \n",
    "#     rcv.fit(X_train_full, y_train_full)\n",
    "#     ftd_model[model.__class__.__name__] = rcv.best_estimator_\n",
    "#     print(rcv.best_score_, rcv.best_estimator_, sep='\\n\\n')\n",
    "\n",
    "# rcv_results = pd.DataFrame(rcv.cv_results_)\n",
    "# rcv_results.to_csv('rcv_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d3e23b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.647879Z",
     "iopub.status.busy": "2023-08-06T17:05:56.647449Z",
     "iopub.status.idle": "2023-08-06T17:05:56.654824Z",
     "shell.execute_reply": "2023-08-06T17:05:56.653648Z"
    },
    "papermill": {
     "duration": 0.02554,
     "end_time": "2023-08-06T17:05:56.657498",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.631958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In separate runs of RandomizedSearchCV, I found following to be the best performing model configurations: \\nGradientBoostingClassifier(learning_rate=0.05, max_depth=7, max_features='log2', n_estimators=154, subsample=0.55, random_state=SEED) , \\n\\nGradientBoostingClassifier(max_depth=7, max_features='sqrt', n_estimators=600, n_iter_no_change=50, tol=0, subsample=0.75, \\nlearning_rate=0.05, validation_fraction=0.2, verbose=0, random_state=SEED) , \\n\\nGradientBoostingClassifier(max_depth=5, max_features='sqrt', n_estimators=300, n_iter_no_change=25, tol=0, subsample=0.75, \\nlearning_rate=0.1, validation_fraction=0.2, verbose=0, random_state=SEED) \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' In separate runs of RandomizedSearchCV, I found following to be the best performing model configurations: \n",
    "GradientBoostingClassifier(learning_rate=0.05, max_depth=7, max_features='log2', n_estimators=154, subsample=0.55, random_state=SEED) , \n",
    "\n",
    "GradientBoostingClassifier(max_depth=7, max_features='sqrt', n_estimators=600, n_iter_no_change=50, tol=0, subsample=0.75, \n",
    "learning_rate=0.05, validation_fraction=0.2, verbose=0, random_state=SEED) , \n",
    "\n",
    "GradientBoostingClassifier(max_depth=5, max_features='sqrt', n_estimators=300, n_iter_no_change=25, tol=0, subsample=0.75, \n",
    "learning_rate=0.1, validation_fraction=0.2, verbose=0, random_state=SEED) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59942c44",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.688615Z",
     "iopub.status.busy": "2023-08-06T17:05:56.688188Z",
     "iopub.status.idle": "2023-08-06T17:05:56.695412Z",
     "shell.execute_reply": "2023-08-06T17:05:56.694201Z"
    },
    "papermill": {
     "duration": 0.025902,
     "end_time": "2023-08-06T17:05:56.697954",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.672052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' I have tried to minimize overfitting through multiple iterations of fitting models on separate subsamples of training \n",
    "data. \n",
    "\n",
    "By splitting training data into train and validation sets, and only fitting on training set, I think that we miss using for \n",
    "fitting the validation data, which is a large part of our available training data. But we cannot also use the whole training \n",
    "set for fitting because we need unseen data for validation. How do we solve this issue? For evaluating model performance, we \n",
    "do this through cross-validation algorithms(like cross_val_score), but I haven't found anything that will use \n",
    "this concept of cross-validated training for predicting on new data. So, I just tried a hacky way to achieve this. \n",
    "I split the full training set into multiple training and validation sets using StratifiedKFold, or StratifiedShuffleSplit, \n",
    "and then used the training split for fitting, validation split to minimize model loss, and then used this trained \n",
    "model to make predictions on the test set. I obtained multiple predictions on test set this way and then used \n",
    "soft-voting (averaging prediction probabilities) to get average prediction probability for each record, which I have \n",
    "eventually used in my model submission. In my trials, I noticed that soft-voting this way gave better prediction \n",
    "scores on kaggle submissions than hard-voting. This approach has worked well most of the times with the submission \n",
    "based on averages giving better kaggle score than any of the individual set of generated predictions. \n",
    "''' \n",
    "\n",
    "pred_dict = dict() # to save separate predictions basis shuffled splits of training data \n",
    "n_splits = 5 # no of shuffled splits of data we will try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232630cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.727619Z",
     "iopub.status.busy": "2023-08-06T17:05:56.727194Z",
     "iopub.status.idle": "2023-08-06T17:05:56.734226Z",
     "shell.execute_reply": "2023-08-06T17:05:56.733022Z"
    },
    "papermill": {
     "duration": 0.024633,
     "end_time": "2023-08-06T17:05:56.736591",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.711958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Exploring the Light Gradient Boosting Classifier \n",
    "# ''' LGBMClassifier has produced almost par results with our tuned sklearn GradientBoostingClassifier on my validation runs, \n",
    "# but in the Kaggle submissions, the sklearn variant has almost always produced a better score, so I have decided to run my \n",
    "# submissions using the sklearn model until something else gives me better results  '''\n",
    "\n",
    "# # LGBMClassifier with early stopping basis validation set \n",
    "# '''Minimizing over-fitting, improving model generalization while using maximum possible training data for fitting our model ''' \n",
    "# sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=SEED, ) \n",
    "\n",
    "# for i, (tr_ids, ts_ids) in enumerate(sss.split(X_train_full, y_train_full)): \n",
    "#     model = LGBMClassifier(random_state=SEED+i, n_estimators=600, learning_rate=0.05, metric='binary_logloss',) \n",
    "#     model = model.fit(X_train_full[tr_ids], y_train_full[tr_ids], early_stopping_rounds=50, eval_set=[(X_train_full[ts_ids], y_train_full[ts_ids])], verbose=0 ) \n",
    "#     # scoring \n",
    "#     print(i, 'best_iteration', model._best_iteration) # early stopping will retain best iteration weights to use for predictions  \n",
    "#     print(i, 'log_loss', log_loss(y_train_full[ts_ids], model.predict_proba(X_train_full[ts_ids])[:,1]))\n",
    "#     print(i, 'accuracy', model.score(X_train_full[ts_ids], y_train_full[ts_ids]), '\\n')\n",
    "#     print()\n",
    "\n",
    "#     # predict on test set \n",
    "#     pred_dict['lgb_'+str(i)] = model.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "# predict_frame = pd.DataFrame(pred_dict)\n",
    "# predict_frame['lgb_avg'] = predict_frame.iloc[:,-n_splits:].mean(axis=1) # soft voting gives better results than hard voting \n",
    "# # predict_frame['gb_avg'] = predict_frame.median(axis=1) # hard voting  \n",
    "# print(predict_frame.describe()) \n",
    "\n",
    "# y_test_pred = np.round(predict_frame['lgb_avg'].values)  \n",
    "# # y_test_pred = np.round(predict_frame['lgb_0'].values)  \n",
    "# print(y_test_pred[:5]) \n",
    "# print(np.mean(y_test_pred)) \n",
    "\n",
    "# submission = pd.DataFrame(dict(PassengerId = test.PassengerId, Transported = y_test_pred)) \n",
    "# submission.Transported = submission.Transported.astype('bool')\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head() \n",
    "    \n",
    "# print('SEED:', SEED) # to see THE random seed used in this run \n",
    "# print ( ((predict_frame > 0.45) & (predict_frame < 0.55)).sum() )\n",
    "\n",
    "# # print(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c31d9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:05:56.768579Z",
     "iopub.status.busy": "2023-08-06T17:05:56.768180Z",
     "iopub.status.idle": "2023-08-06T17:06:27.418098Z",
     "shell.execute_reply": "2023-08-06T17:06:27.416973Z"
    },
    "papermill": {
     "duration": 30.683126,
     "end_time": "2023-08-06T17:06:27.435420",
     "exception": false,
     "start_time": "2023-08-06T17:05:56.752294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak oob cum. improv:  0.5496187840898323 0.5496187840898323\n",
      "Before n_est tuning: 0: 0.5293990643590882\n",
      "Peak oob cum. improv:  0.5496187840898323 0.5496187840898323\n",
      "After n_est tuning: 0: 0.5406349575724206\n",
      "Peak oob cum. improv:  0.5446580913601031 0.5446580913601031\n",
      "Before n_est tuning: 1: 0.4983053493158465\n",
      "Peak oob cum. improv:  0.5446580913601031 0.5446580913601031\n",
      "After n_est tuning: 1: 0.5103436336579543\n",
      "Peak oob cum. improv:  0.5446520325322043 0.5446520325322043\n",
      "Before n_est tuning: 2: 0.5278349528219952\n",
      "Peak oob cum. improv:  0.5446520325322043 0.5446520325322043\n",
      "After n_est tuning: 2: 0.537587234546397\n",
      "Peak oob cum. improv:  0.5361552155796631 0.5361552155796631\n",
      "Before n_est tuning: 3: 0.49363342760165524\n",
      "Peak oob cum. improv:  0.5361552155796631 0.5361552155796631\n",
      "After n_est tuning: 3: 0.5090312418921212\n",
      "Peak oob cum. improv:  0.5483452090187825 0.5483452090187825\n",
      "Before n_est tuning: 4: 0.5364464524288339\n",
      "Peak oob cum. improv:  0.5483452090187825 0.5483452090187825\n",
      "After n_est tuning: 4: 0.5466644146096543\n",
      "             gbc_0        gbc_1        gbc_2        gbc_3        gbc_4  \\\n",
      "count  4277.000000  4277.000000  4277.000000  4277.000000  4277.000000   \n",
      "mean      0.502756     0.502083     0.502259     0.502935     0.503360   \n",
      "std       0.346027     0.356978     0.349424     0.354224     0.342136   \n",
      "min       0.002767     0.002119     0.002771     0.002989     0.005336   \n",
      "25%       0.139362     0.123823     0.132577     0.131115     0.144099   \n",
      "50%       0.514333     0.507147     0.517032     0.498446     0.520024   \n",
      "75%       0.859472     0.878006     0.863981     0.887924     0.856338   \n",
      "max       0.994014     0.996763     0.993214     0.997252     0.990801   \n",
      "\n",
      "            gb_avg  \n",
      "count  4277.000000  \n",
      "mean      0.502679  \n",
      "std       0.347090  \n",
      "min       0.003900  \n",
      "25%       0.136937  \n",
      "50%       0.515615  \n",
      "75%       0.862270  \n",
      "max       0.993390  \n",
      "[1. 0. 1. 1. 1.]\n",
      "0.5104044891278934\n",
      "SEED: 66451\n",
      "gbc_0     334\n",
      "gbc_1     279\n",
      "gbc_2     300\n",
      "gbc_3     322\n",
      "gbc_4     325\n",
      "gb_avg    336\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Tuned GradientBoostingClassifier with early stopping basis validation set \n",
    "'''Minimizing over-fitting, improving model generalization while using maximum possible training data for fitting our model ''' \n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=2, random_state=SEED, ) \n",
    "\n",
    "for i, (tr_ids, ts_ids) in enumerate(sss.split(X_train_full, y_train_full)): \n",
    "    # training on shuffled dataset with 0.2 validation_fraction \n",
    "    model = GradientBoostingClassifier(max_depth=7, max_features='sqrt', n_estimators=600, n_iter_no_change=50, tol=0, subsample=0.75, learning_rate=0.05, validation_fraction=0.2, verbose=0, random_state=SEED+i )\n",
    "    model = model.fit(X_train_full[tr_ids], y_train_full[tr_ids] ) \n",
    "    # Checking OOB scores \n",
    "    raw_ests = model.n_estimators_ \n",
    "    opt_ests = np.argmax(np.cumsum(model.oob_improvement_)) + 1 \n",
    "    estop_ests = raw_ests - model.n_iter_no_change \n",
    "#     print(opt_ests, raw_ests, sep=' / ' )\n",
    "    print('Peak oob cum. improv: ', np.max(np.cumsum(model.oob_improvement_)), np.cumsum(model.oob_improvement_)[opt_ests-1] )\n",
    "    print('Before n_est tuning', i, np.sum(model.oob_improvement_), sep=': ')\n",
    "#     print(i, 'log_loss', log_loss(y_train_full[ts_ids], model.predict_proba(X_train_full[ts_ids])[:,1])) \n",
    "#     print(i, 'accuracy', model.score(X_train_full[ts_ids], y_train_full[ts_ids]), '\\n') \n",
    "    \n",
    "    ''' To refit the model using n_estimators corresponding to peak cum. improvement, use n_estimators=opt_ests '''\n",
    "    \n",
    "    # Refit the model using n_estimators = raw_ests - early stopping \n",
    "    model = GradientBoostingClassifier(max_depth=7, max_features='sqrt', n_estimators=estop_ests, n_iter_no_change=50, tol=0, subsample=0.75, learning_rate=0.05, validation_fraction=0.2, verbose=0, random_state=SEED+i) \n",
    "    model = model.fit(X_train_full[tr_ids], y_train_full[tr_ids]) \n",
    "    # Checking OOB scores \n",
    "    opt_ests = np.argmax(np.cumsum(model.oob_improvement_)) + 1 \n",
    "#     print(opt_ests, model.n_estimators_, sep=' / ' )\n",
    "    print('Peak oob cum. improv: ', np.max(np.cumsum(model.oob_improvement_)), np.cumsum(model.oob_improvement_)[opt_ests-1] )\n",
    "    print('After n_est tuning', i, np.sum(model.oob_improvement_), sep=': ')\n",
    "#     print(i, 'log_loss', log_loss(y_train_full[ts_ids], model.predict_proba(X_train_full[ts_ids])[:,1])) \n",
    "#     print(i, 'accuracy', model.score(X_train_full[ts_ids], y_train_full[ts_ids]), '\\n') \n",
    "\n",
    "    # predict on test set using model trained on StratifiedShuffleSplit of training data with 0.2 validation_fraction \n",
    "    pred_dict['gbc_'+str(i)] = model.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "predict_frame = pd.DataFrame(pred_dict)\n",
    "predict_frame['gb_avg'] = predict_frame.iloc[:,-n_splits:].mean(axis=1) # soft voting gives better results than hard voting \n",
    "# predict_frame['gb_avg'] = predict_frame.median(axis=1) # hard voting  \n",
    "print(predict_frame.describe()) \n",
    "\n",
    "y_test_pred = np.round(predict_frame['gb_avg'].values)  \n",
    "# y_test_pred = np.round(predict_frame[2].values)  \n",
    "print(y_test_pred[:5]) \n",
    "print(np.mean(y_test_pred)) \n",
    "\n",
    "submission = pd.DataFrame(dict(PassengerId = test.PassengerId, Transported = y_test_pred)) \n",
    "submission.Transported = submission.Transported.astype('bool')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head() \n",
    "\n",
    "print('SEED:', SEED) # to see THE random seed used in this run \n",
    "print ( ((predict_frame > 0.45) & (predict_frame < 0.55)).sum() )\n",
    "# print(model.__dict__) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c1c25d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.467961Z",
     "iopub.status.busy": "2023-08-06T17:06:27.466660Z",
     "iopub.status.idle": "2023-08-06T17:06:27.475262Z",
     "shell.execute_reply": "2023-08-06T17:06:27.474072Z"
    },
    "papermill": {
     "duration": 0.027193,
     "end_time": "2023-08-06T17:06:27.477568",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.450375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In the next few code blocks, I have explored Keras deep learning algorithm to sovle our binary prediction problem. The tuned version \\nof this model has, like LightGBM, produced almost similar results as our tuned GradientBoostingClassifier, but has almost always \\ncome a close 2nd or 3rd. Plus, this one takes about 10-15 minutes to run compared to just a few minutes for other models. That said, \\nI really got to learn much about concepts such as dropout rate, batch normalization between iterations, how algorithms try to minimize loss \\nfunctions, and how early stopping works, and much more while working on this keras model. And then I used those insights to further tune \\nmy GradientBoostingClassifier and LGBMClassifer for better performance. '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Keras neural network \n",
    "\n",
    "''' In the next few code blocks, I have explored Keras deep learning algorithm to sovle our binary prediction problem. The tuned version \n",
    "of this model has, like LightGBM, produced almost similar results as our tuned GradientBoostingClassifier, but has almost always \n",
    "come a close 2nd or 3rd. Plus, this one takes about 10-15 minutes to run compared to just a few minutes for other models. That said, \n",
    "I really got to learn much about concepts such as dropout rate, batch normalization between iterations, how algorithms try to minimize loss \n",
    "functions, and how early stopping works, and much more while working on this keras model. And then I used those insights to further tune \n",
    "my GradientBoostingClassifier and LGBMClassifer for better performance. ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ede7bede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.510272Z",
     "iopub.status.busy": "2023-08-06T17:06:27.509567Z",
     "iopub.status.idle": "2023-08-06T17:06:27.515404Z",
     "shell.execute_reply": "2023-08-06T17:06:27.514581Z"
    },
    "papermill": {
     "duration": 0.024583,
     "end_time": "2023-08-06T17:06:27.517552",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.492969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Building keras deep learning model \n",
    "\n",
    "# from tensorflow.keras.models import Sequential \n",
    "# from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU \n",
    "# from tensorflow.keras.optimizers import Adam, SGD \n",
    "# from tensorflow.keras.callbacks import EarlyStopping \n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "# # from scikeras.wrappers import KerasClassifier # this isn't working here \n",
    "# set_random_seed = SEED \n",
    "\n",
    "# input_shape = (X_train_full.shape[1], ) \n",
    "\n",
    "# # create function to return deep learning model \n",
    "# def create_model(input_shape=input_shape, n_layers=1, n_nodes=24, dropout_rate=0.3, \n",
    "#                  activation='sigmoid', optimizer=Adam, learning_rate=0.001): \n",
    "#     dropout_rate = dropout_rate / n_layers \n",
    "#     model = Sequential() \n",
    "#     model.add(BatchNormalization(input_shape=input_shape) ) # input layer outside loop \n",
    "#     for layer in range(n_layers):\n",
    "#         model.add(Dense(units=n_nodes, activation=activation) ) \n",
    "#         model.add(Dropout(rate=dropout_rate)) \n",
    "#         model.add(BatchNormalization() ) \n",
    "#     model.add(Dense(units=1, activation='sigmoid'))\t# output layer outside loop with sigmoid activation for binary classification \n",
    "#     model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "#     return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e86089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.549205Z",
     "iopub.status.busy": "2023-08-06T17:06:27.548750Z",
     "iopub.status.idle": "2023-08-06T17:06:27.555208Z",
     "shell.execute_reply": "2023-08-06T17:06:27.554072Z"
    },
    "papermill": {
     "duration": 0.025037,
     "end_time": "2023-08-06T17:06:27.557369",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.532332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # tuning our keras model hyper-parameters with RandomizedSearchCV\n",
    "# set_random_seed = SEED \n",
    "\n",
    "# # Convert keras model into an estimator usable with sklearn RandomizedSearchCV \n",
    "# model = KerasClassifier(build_fn=create_model, verbose=1, ) \n",
    "\n",
    "# # providing training and test set indices from train_test_split run previously to be used for model validation in \n",
    "# # RandomizedSearchCV as well as a validation set for early stopping in each run of keras model \n",
    "# cv = [(tts_tr, tts_val)]\n",
    "\n",
    "# # define parameters dictionary for hyperparameter tuning \n",
    "# param_dist = dict(n_layers=np.arange(1,4),n_nodes=np.arange(16, 33, 4), dropout_rate=[0.2, 0.3, 0.4], \n",
    "#                   activation=['relu', LeakyReLU(alpha=0.1), 'tanh'], # batch_size=[16, 32, 64], \n",
    "#                   optimizer=[Adam, SGD], learning_rate=[0.001, 0.01, 0.1], # epochs=np.arange(50, 201), \n",
    "#                   ) \n",
    "# # Instantiate RandomizedSearchCV \n",
    "# rscv = RandomizedSearchCV(estimator=model, param_distributions=param_dist, random_state=SEED, \n",
    "#                           n_iter=72, scoring='neg_log_loss', cv=cv, # n_jobs=-1, \n",
    "#                           verbose=2) \n",
    "\n",
    "# # define early stopping parameters \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=16, restore_best_weights=True, )\n",
    "\n",
    "# # Run RandomizedSearchCV using training data & validation set for early stopping \n",
    "# rscv.fit(X_train_full, y_train_full, validation_data=(X_val, y_val), \n",
    "#          batch_size=32, epochs=150, callbacks=[early_stopping] ) \n",
    "\n",
    "# # Print results\n",
    "# print('best params: ', rscv.best_params_, sep='\\n') \n",
    "# print('best score: ', rscv.best_score_, sep='\\n') \n",
    "\n",
    "# y_predprob_val = rscv.best_estimator_.predict_proba(X_val)[:,1] \n",
    "# val_scores = [log_loss(y_val, y_predprob_val), \n",
    "#               roc_auc_score(y_val, y_predprob_val), \n",
    "#               accuracy_score(y_val, np.round(y_predprob_val))] \n",
    "\n",
    "# print('best scores on val set: ', val_scores , sep='\\n') \n",
    "# rscv_results = pd.DataFrame(rscv.cv_results_)\n",
    "# # pd.set_option('display.max_rows', 75) \n",
    "# # print(rscv_results) \n",
    "# rscv_results.to_csv('rscv_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "860971a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.589368Z",
     "iopub.status.busy": "2023-08-06T17:06:27.588236Z",
     "iopub.status.idle": "2023-08-06T17:06:27.596770Z",
     "shell.execute_reply": "2023-08-06T17:06:27.595761Z"
    },
    "papermill": {
     "duration": 0.026936,
     "end_time": "2023-08-06T17:06:27.599013",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.572077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In separate runs of RandomizedSearchCV with my keras model, I found following to be \\namong the best performing model configurations, with the top one being the best of all: \\n\\ncreate_model(n_layers=1, n_nodes=16, dropout_rate=0.2, activation='tanh', optimizer=Adam, learning_rate=0.01,) \\ncreate_model(n_layers=1, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\ncreate_model(n_layers=1, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\ncreate_modeln_layers=1, n_nodes=24, dropout_rate=0.3, activation='sigmoid', optimizer=Adam, learning_rate=0.001) \\n\\ncreate_model(n_layers=2, n_nodes=16, dropout_rate=0.3, activation='tanh', optimizer=Adam, learning_rate=0.01,) \\ncreate_model(n_layers=2, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\ncreate_model(n_layers=2, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\n\\ncreate_model(n_layers=3, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\ncreate_model(n_layers=3, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' In separate runs of RandomizedSearchCV with my keras model, I found following to be \n",
    "among the best performing model configurations, with the top one being the best of all: \n",
    "\n",
    "create_model(n_layers=1, n_nodes=16, dropout_rate=0.2, activation='tanh', optimizer=Adam, learning_rate=0.01,) \n",
    "create_model(n_layers=1, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "create_model(n_layers=1, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "create_modeln_layers=1, n_nodes=24, dropout_rate=0.3, activation='sigmoid', optimizer=Adam, learning_rate=0.001) \n",
    "\n",
    "create_model(n_layers=2, n_nodes=16, dropout_rate=0.3, activation='tanh', optimizer=Adam, learning_rate=0.01,) \n",
    "create_model(n_layers=2, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "create_model(n_layers=2, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "\n",
    "create_model(n_layers=3, n_nodes=20, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "create_model(n_layers=3, n_nodes=24, dropout_rate=0.2, activation='relu', optimizer=Adam, learning_rate=0.01,) \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112a195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.631097Z",
     "iopub.status.busy": "2023-08-06T17:06:27.630450Z",
     "iopub.status.idle": "2023-08-06T17:06:27.637502Z",
     "shell.execute_reply": "2023-08-06T17:06:27.636396Z"
    },
    "papermill": {
     "duration": 0.025827,
     "end_time": "2023-08-06T17:06:27.639908",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.614081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fitting multiple subsamples of training data on tuned keras neural network model to make predictions \n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED, ) \n",
    "# # skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=SEED, ) \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, )\n",
    "# history_frame = pd.DataFrame() \n",
    "# # pred_dict = dict() \n",
    "\n",
    "# for i, (tr_ids, ts_ids) in enumerate(skf.split(X_train_full, y_train_full)): \n",
    "#     # training on cross-validation split \n",
    "#     set_random_seed = SEED + i \n",
    "#     model = create_model(n_layers=1, n_nodes=16, dropout_rate=0.2, activation='tanh', \n",
    "#                          optimizer=Adam, learning_rate=0.01,) \n",
    "#     history = model.fit(X_train_full[tr_ids], y_train_full[tr_ids], epochs=200, \n",
    "#                         batch_size=32, callbacks=[early_stopping], \n",
    "#                         validation_data=[X_train_full[ts_ids], y_train_full[ts_ids]]) \n",
    "#     history_frame = history_frame.append(pd.DataFrame({**history.history, 'split': i}), ignore_index=True) \n",
    "#     # predict on test set using model trained on cross-validated subset of training data \n",
    "#     pred_dict['nn_'+str(i)] = model.predict(X_test).flatten() \n",
    "\n",
    "# print(history_frame.nsmallest(20, 'val_loss'))\n",
    "# print('Above values sorted by val_loss, below values sorted by val_accuracy')\n",
    "# print(history_frame.nlargest(20, 'val_binary_accuracy'))\n",
    "\n",
    "# predict_frame = pd.DataFrame(pred_dict)\n",
    "# predict_frame['nn_avg'] = predict_frame.iloc[:, -skf.n_splits:].mean(axis=1) # final prediction submission using soft voting \n",
    "# predict_frame['nn_avg'] = predict_frame.iloc[:, -skf.n_splits:].median(axis=1) # submission using hard voting -> isn't good as soft voting \n",
    "# print(predict_frame.describe()) \n",
    "\n",
    "# y_test_pred = np.round(predict_frame['nn_avg'].values)  \n",
    "# # y_test_pred = np.round(predict_frame[2].values)  \n",
    "# print(y_test_pred[:5]) \n",
    "# print(np.mean(y_test_pred)) \n",
    "\n",
    "# submission = pd.DataFrame(dict(PassengerId = test.PassengerId, Transported = y_test_pred)) \n",
    "# submission.Transported = submission.Transported.astype('bool')\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14148256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.672408Z",
     "iopub.status.busy": "2023-08-06T17:06:27.671953Z",
     "iopub.status.idle": "2023-08-06T17:06:27.677462Z",
     "shell.execute_reply": "2023-08-06T17:06:27.676298Z"
    },
    "papermill": {
     "duration": 0.024246,
     "end_time": "2023-08-06T17:06:27.679638",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.655392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''Predicting basis soft voting on multiple algorithm predictions - LGBMClassifier, GradientBoostingClassifer, \n",
    "# and Keras NN when all are activated in previous code blocks '''\n",
    "# '''kaggle best score for this block so far 0.8036 V31 '''\n",
    "\n",
    "# predict_frame = pd.DataFrame(pred_dict)\n",
    "# predict_frame['all_avg'] = predict_frame.mean(axis=1) \n",
    "# print(predict_frame.describe()) \n",
    "\n",
    "# y_test_pred = np.round(predict_frame['all_avg'].values)  \n",
    "# # y_test_pred = np.round(predict_frame[2].values)  \n",
    "# print(y_test_pred[:5]) \n",
    "# print(np.mean(y_test_pred)) \n",
    "\n",
    "# submission = pd.DataFrame(dict(PassengerId = test.PassengerId, Transported = y_test_pred)) \n",
    "# submission.Transported = submission.Transported.astype('bool')\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head() \n",
    "\n",
    "# print('SEED:', SEED) # to see THE random seed used in this run \n",
    "# print ( ((predict_frame > 0.45) & (predict_frame < 0.55)).sum() ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97843360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T17:06:27.711935Z",
     "iopub.status.busy": "2023-08-06T17:06:27.711510Z",
     "iopub.status.idle": "2023-08-06T17:06:27.716797Z",
     "shell.execute_reply": "2023-08-06T17:06:27.715650Z"
    },
    "papermill": {
     "duration": 0.024116,
     "end_time": "2023-08-06T17:06:27.719333",
     "exception": false,
     "start_time": "2023-08-06T17:06:27.695217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Use this code block to generate a quick submission using any specified model \n",
    "# set_random_seed = SEED \n",
    "# # y_test_pred = # place model definition here \n",
    "# y_test_pred = GradientBoostingClassifier(learning_rate=0.05, max_depth=7, max_features='log2', n_estimators=154, \n",
    "#                                          subsample=0.55).fit(X_train_full, y_train_full).predict(X_test) # 0.80406 V15 \n",
    "# print(np.mean(y_test_pred)) \n",
    "# submission = pd.DataFrame(dict(PassengerId = test.PassengerId, Transported = y_test_pred)) \n",
    "# submission.Transported = submission.Transported.astype('bool')\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.760937,
   "end_time": "2023-08-06T17:06:30.696923",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-06T17:05:26.935986",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
